import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import urllib
from serpapi import GoogleSearch
import plotly.express as px
import plotly
import base64
import os
from pymed import PubMed
from dotenv import load_dotenv
from nasaplooza.utils.log import logger

load_dotenv()

pubmed = PubMed(tool="Searcher", email="my@email.address")

headers = {
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36"
}

SERP_API_KEY = os.getenv("SERP_API_KEY", "")


def scrape_paper_info(paper_name):
    params = {"engine": "google_scholar", "q": paper_name, "api_key": SERP_API_KEY}
    year_pattern = r"\b\d{4}\b"
    search = GoogleSearch(params)
    results = search.get_dict()
    organic_results = results.get("organic_results", [])
    if results.get("search_metadata", {}).get("status") == "Success":
        paper_info = {
            "Paper Title": [],
            "Paper Abstract": [],
            "Year": [],
            "Author": [],
            "Citation": [],
            "Publication": [],
            "Url of paper": [],
        }
        try:
            related_searches = [
                rs.get("query", None) for rs in results.get("related_searches", [])
            ]
        except Exception as e:
            print("related searches", e)
            related_searches = []
        author_ids = []
        for result in organic_results:
            title = result.get("title", "")
            authors = ", ".join(
                [
                    each_author.get("name")
                    for each_author in result.get("publication_info").get("authors", [])
                ]
            )
            abstract = result.get("snippet", "").strip()
            link = result.get("link", "")
            year_matches = re.findall(
                year_pattern, result.get("publication_info", {}).get("summary", "")
            )
            year = year_matches[0] if len(year_matches) > 0 else None
            paper_citations = (
                result.get("inline_links", {}).get("cited_by", {}).get("total", None)
            )
            publication = (
                result.get("publication_info", {}).get("summary", "").split("-")[-1]
            )
            paper_info["Paper Title"].append(title)
            paper_info["Author"].append(authors)
            paper_info["Paper Abstract"].append(abstract)
            paper_info["Url of paper"].append(link)
            paper_info["Year"].append(year)
            paper_info["Citation"].append(paper_citations)
            paper_info["Publication"] = publication
            try:
                author_ids += [
                    each_author.get("author_id")
                    for each_author in result.get("publication_info").get("authors", [])
                ]
            except Exception as E:
                print(E)
                author_ids += []
        df = pd.DataFrame(paper_info)
        author_profiles = []
        for each_author_id in author_ids:
            params = {
                "engine": "google_scholar_author",
                "author_id": each_author_id,
                "api_key": SERP_API_KEY,
            }
            search = GoogleSearch(params)
            results = search.get_dict()
            author = results.get("author")
            for row in results.get("cited_by").get("table"):
                if "citations" in row:
                    author["citations"] = row.get("citations").get("all")
                if "h_index" in row:
                    author["h_index"] = row.get("h_index").get("all")
                if "i10_index" in row:
                    author["i10_index"] = row.get("i10_index").get("all")
            try:
                author["graph"] = convert_graph_to_base64(
                    px.bar(
                        pd.DataFrame(results.get("cited_by").get("graph")),
                        x="year",
                        y="citations",
                    )
                )
                author_profiles.append(author)
            except ValueError:
                pass
        return df, author_profiles, related_searches


def convert_graph_to_base64(fig):
    png = plotly.io.to_image(fig, format="png")
    png_base64 = base64.b64encode(png).decode("ascii")
    return png_base64


def get_total_papers_from_gs(query):
    query = query.replace(" ", "+")
    base_url = f"https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q={query}&oq="
    response = requests.get(base_url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")
        total = soup.find_all("div", {"class": "gs_ab_mdw"}).text.strip()
        total = total.replace(",", "")
        total = re.findall(r"\d+", total)
        return int(total[0])
    return 0


def get_total_papers_from_arxiv(query):
    query = query.replace(" ", "+")
    base_url = f"https://link.springer.com/search?query={query}"
    response = requests.get(base_url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")
        total = soup.find_all("div", {"class": "gs_ab_mdw"}).text.strip()
        total = total.replace(",", "")
        total = re.findall(r"\d+", total)
        return int(total[0])
    return 0


def scrape_arxiv(query):
    # Create the URL for the arXiv search
    query = query.replace(" ", "+")
    base_url = f"https://arxiv.org/search/?query={query}&searchtype=all&abstracts=show&size=200"
    response = requests.get(base_url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")
        # Extract data from the search results
        titles = []
        authors = []
        abstracts = []
        links = []
        years = []
        for result in soup.find_all("li", class_="arxiv-result"):
            title = result.find("p", class_="title is-5 mathjax")
            author = result.find("p", class_="authors").find("a", href=True)
            abstract = result.find("span", class_="abstract-full")
            link = result.find("p", class_="list-title").find("a", href=True)
            year = result.find("p", class_="is-size-7")
            # if title and author and abstract and link:
            titles.append(title.text.strip())
            authors.append(author.text.strip())
            abstracts.append(abstract.text.strip().replace("â–³ Less", ""))
            links.append(link["href"].replace("/abs/", "/pdf/"))
            years.append(year.text.strip()[-5:])

        # Create a DataFrame
        data = {
            "Paper Title": titles,
            "Author": authors,
            "Paper Abstract": abstracts,
            "Url of paper": links,
            "Year": years,
        }
        df = pd.DataFrame(data)
        df["Publication"] = "arxiv"
        df["Citation"] = "-"
        return df.head(10)
    else:
        print("Error: Unable to fetch data from arXiv")
        return None


# this function for the getting inforamtion of the web page
def get_paperinfo(paper_url):

    # download the page
    response = requests.get(paper_url, headers=headers)

    # check successful response
    if response.status_code != 200:
        print("Status code:", response.status_code)
        raise Exception("Failed to fetch web page ")

    # parse using beautiful soup
    paper_doc = BeautifulSoup(response.text, "html.parser")

    return paper_doc


# this function for the extracting information of the tags
def get_tags(doc):
    paper_tag = doc.select("[data-lid]")
    cite_tag = doc.find_all("div", {"class": "gs_fl"})  # doc.select('[title=Cite] + a')
    link_tag = doc.find_all("div", {"class": "gs_or_ggsm"})
    author_tag = doc.find_all("div", {"class": "gs_a"})
    abstract_tag = doc.find_all("div", {"class": "gs_rs"})
    return paper_tag, cite_tag, link_tag, author_tag, abstract_tag


# it will return the title of the paper
def get_papertitle(paper_tag):

    paper_names = []

    for tag in paper_tag:
        paper_names.append(tag.select("h3")[0].get_text())

    return paper_names


# it will return the number of citation of the paper
def get_citecount(cite_tag):
    cite_count = []
    for i in cite_tag:
        cite = i.text
        res = re.findall(r"Cited by\s*(-?\d+(?:\.\d+)?)", cite)
        cite_count.extend(res)
    return cite_count


# function for the getting link information
def get_link(link_tag):

    links = []

    for i in range(len(link_tag)):
        links.append(link_tag[i].a["href"])
    return links


# function for the getting autho , year and publication information
def get_author_year_publi_info(authors_tag):
    years = []
    publication = []
    authors = []
    for i in range(len(authors_tag)):
        authortag_text = (authors_tag[i].text).split()
        year = int(re.search(r"\d+", authors_tag[i].text).group())
        years.append(year)
        publication.append(authortag_text[-1])
        author = ",".join(authors_tag[i].text.split(",")[:-1])
        # author = authortag_text[0] + ' ' + re.sub(',','', authortag_text[1])
        authors.append(author)

    return years, publication, authors


def get_paperabstract(abstract_tag):

    abstracts = []

    for i in range(len(abstract_tag)):
        abstract_text = (abstract_tag[i].text).split()
        abstracts.append(" ".join(abstract_text))

    return abstracts


def get_top_publication_topics():
    url = "https://scholar.google.com/citations?view_op=top_venues"
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")
        # Extract data from the search results
        publication = []
        h5index = []
        for result in soup.find_all("tr"):
            title = result.find("td", class_="gsc_mvt_t")
            index = result.find("td", class_="gsc_mvt_n").find(
                "a", class_="gs_ibl gsc_mp_anchor"
            )
            publication.append(title.text.strip())
            h5index.append(index.text.strip())


def get_all_papers_from_scholar(query, start):
    final_df = pd.DataFrame()
    # creating final repository
    paper_repos_dict = {
        "Paper Title": [],
        "Paper Abstract": [],
        "Year": [],
        "Author": [],
        "Citation": [],
        "Publication": [],
        "Url of paper": [],
    }

    query = query.replace(" ", "+").lower()
    # get url for the each page
    url = "https://scholar.google.com/scholar?start={}&q={}&hl=en&as_sdt=0,10".format(
        start, query
    )
    # url = "https://scholar.google.com/scholar?&q={}+&hl=en".format(query)

    # function for the get content of each page
    doc = get_paperinfo(url)

    # function for the collecting tags
    paper_tag, cite_tag, link_tag, author_tag, abstract_tag = get_tags(doc)

    # print("Citation tag",cite_tag)
    # paper title from each page
    papername = get_papertitle(paper_tag)

    paperabstract = get_paperabstract(abstract_tag)

    # year , author , publication of the paper
    year, publication, author = get_author_year_publi_info(author_tag)

    # cite count of the paper
    cite = get_citecount(cite_tag)

    # url of the paper
    link = get_link(link_tag)

    final_df = pd.concat(
        [
            final_df,
            pd.DataFrame(
                {
                    "Paper Title": pd.Series(papername),
                    "Paper Abstract": pd.Series(paperabstract),
                    "Year": pd.Series(year),
                    "Author": pd.Series(author),
                    "Citation": pd.Series(cite),
                    "Publication": pd.Series(publication),
                    "Url of paper": pd.Series(link),
                }
            ),
        ],
        ignore_index=True,
    )
    return final_df


def get_author_details(url: str):
    URL = "https://scholar.google.com"
    page = requests.get(URL + url)
    soup = BeautifulSoup(page.content, "html.parser")
    print(soup)
    name = soup.find_all("div", {"class": "gsc_prf_in"})
    print(name)


def get_authors_profiles(query: str) -> list[str]:
    keyword = urllib.parse.quote(query)
    URL = f"https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q={keyword}"
    page = requests.get(URL)
    soup = BeautifulSoup(page.content, "html.parser")
    results = soup.find_all("div", {"class": "gs_a"})
    authors = []
    for res in results:
        for author in res.find_all("a", href=True):
            authors.append(
                str(author)[str(author).find("href=") + 6 : str(author).find('">')]
            )
    print(authors)
    return authors


def get_timeseries_graph(trend_data) -> str:
    data = []
    for d in trend_data:
        data.append(
            {"months": d["date"], "total_queries": d["values"][0]["extracted_value"]}
        )
    if len(data) == 0:
        return ""
    df = pd.DataFrame(data)
    logger.debug("Creating timeseries graph")
    try:
        fig = px.line(df, x="months", y="total_queries")
        png_base64 = convert_graph_to_base64(fig)
    except ValueError:
        png_base64 = ""
    return png_base64


def get_trend_of_title(query: str) -> dict[str, str]:
    response = {}
    params = {
        "engine": "google_trends",
        "q": query,
        "data_type": "TIMESERIES",
        "api_key": SERP_API_KEY,
    }
    logger.debug("Getting trend of the title")
    search = GoogleSearch(params)
    search_results = search.get_dict()
    if "interest_over_time" in search_results:
        response["TIMESERIES"] = search_results["interest_over_time"]
    else:
        response["TIMESERIES"] = {"timeline_data": []}
    timeseries_base = get_timeseries_graph(response["TIMESERIES"]["timeline_data"])
    response = {"timeseries": timeseries_base, "reason": response["TIMESERIES"]}
    return response


def get_citations_by_author(
    author_id, saved_authors: list = [], cited_papers: list = [], depth: int = 3
):
    params = {
        "api_key": SERP_API_KEY,
        "engine": "google_scholar_author",
        "hl": "en",
        "author_id": author_id,
    }
    search = GoogleSearch(params)
    results = search.get_dict()
    artricles = results.get("articles")
    i = 0
    paper_details = []
    for temp in range(1, 20):
        try:
            first_author = artricles[i]["authors"].split(",")[0]
            # print(cited_papers)
            # print(first_author, saved_authors)
            if first_author not in saved_authors:
                paper_details.append(
                    {
                        "title": artricles[i]["title"],
                        "link": artricles[i]["link"],
                        "authors": artricles[i]["authors"],
                    }
                )
                cited_papers.append(artricles[i]["title"])
                saved_authors.append(first_author)
            i += 1
        except Exception as e:
            # print("Failure in get_citations_by_author")
            # print(e)
            continue
        if len(paper_details) >= depth or i >= len(artricles):
            # print(i)
            break
    return paper_details, cited_papers, saved_authors


def get_authorid_by_paper(
    paper_name: str,
    first_author: bool = True,
    return_url: bool = False,
    return_details: bool = False,
):
    params = {
        "engine": "google_scholar",
        "q": f"source: {paper_name}",
        "hl": "en",
        "api_key": SERP_API_KEY,
    }
    search = GoogleSearch(params)
    results = search.get_dict()
    organic_results = results.get("organic_results")[0]
    if first_author:
        try:
            author_id = results.get("organic_results")[0]["publication_info"][
                "authors"
            ][0]["author_id"]
            author_name = results.get("organic_results")[0]["publication_info"][
                "authors"
            ][0]["name"]
        except KeyError:
            author_id = ""
            author_name = ""
    else:
        author_id = results.get("organic_results")[0]["publication_info"]["authors"][
            -1
        ]["author_id"]
        author_name = results.get("organic_results")[0]["publication_info"]["authors"][
            -1
        ]["name"]
    if return_details:
        # print(organic_results['publication_info']["authors"])
        return (
            author_id,
            author_name,
            organic_results["link"],
            ", ".join(
                [i["name"] for i in organic_results["publication_info"]["authors"]]
            ),
        )
    return author_id, author_name


def create_node(name, author, url):
    # Helper function to create a node in the desired format
    return {
        "name": name,
        "attributes": {
            "author": author,
            # "url": url
        },
        "children": [],
    }


def get_map_by_title(title: str, tree_width: int = 3, tree_depth: int = 3):
    saved_authors = []
    cited_papers = [title]
    author_id, author_name, init_links, init_authors = get_authorid_by_paper(
        title, return_details=True
    )
    saved_authors.append(author_name)
    mind_map = create_node(title, author_name, init_links)
    child_citations_papers, cited_papers, saved_authors = get_citations_by_author(
        author_id, saved_authors, cited_papers, tree_depth
    )
    for idx, each_child in enumerate(child_citations_papers):
        author_id, author_name = get_authorid_by_paper(each_child["title"])
        saved_authors.append(author_name)
        child_node = create_node(each_child["title"], author_name, each_child["link"])
        mind_map["children"].append(child_node)
        grand_child_citations_papers, cited_papers, saved_authors = (
            get_citations_by_author(author_id, saved_authors, cited_papers, tree_depth)
        )
        for each_grand_child in grand_child_citations_papers:
            print(each_grand_child, "\n\n\n\n")
            child_node = create_node(
                each_grand_child["title"],
                each_grand_child["title"].split(",")[0],
                each_grand_child["link"],
            )
            mind_map["children"][idx]["children"].append(child_node)
    return mind_map


def scrape_pubmed_paper_info(query: str):
    results = pubmed.query(query, max_results=10)
    results = list(results)

    if len(results) > 0:
        paper_info = {
            "Paper Title": [],
            "Paper Abstract": [],
            "Year": [],
            "Author": [],
            "Citation": [],
            "Publication": [],
            "Url of paper": [],
        }
        print("in if")
        for article in results:
            try:
                paper_info["Paper Title"].append(article.title)
                paper_info["Paper Abstract"].append(article.abstract)
                paper_info["Year"].append(article.publication_date.year)
                authors = []
                for author in article.authors:
                    if author["firstname"] is None:
                        authors.append("")
                    else:
                        authors.append(f"{author['firstname']} {author['lastname']}, ")
                paper_info["Author"].append(authors)
                paper_info["Citation"].append("--")
                paper_info["Publication"].append(article.journal)
                paper_info["Url of paper"].append(None)
            except Exception as e:
                print(e)
                continue
        df = pd.DataFrame(paper_info)
        return df, None, None
